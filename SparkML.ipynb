{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Cloning into 'HMP_Dataset'...\nremote: Enumerating objects: 865, done.\u001b[K\nremote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\nReceiving objects: 100% (865/865), 1010.96 KiB | 4.93 MiB/s, done.\nChecking out files: 100% (848/848), done.\n"
                }
            ], 
            "source": "!git clone https://github.com/wchill/HMP_Dataset.git"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Brush_teeth\tDrink_glass  Liedown_bed  Sitdown_chair  final.py\r\nClimb_stairs\tEat_meat     MANUAL.txt   Standup_chair  impdata.py\r\nComb_hair\tEat_soup     Pour_water   Use_telephone\r\nDescend_stairs\tGetup_bed    README.txt   Walk\r\n"
                }
            ], 
            "source": "! ls HMP_Dataset"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\r\nAccelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\r\nAccelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\r\nAccelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\r\nAccelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\r\nAccelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\r\nAccelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\r\nAccelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\r\n"
                }
            ], 
            "source": "!ls HMP_Dataset/Brush_teeth"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.types import StructType, StructField, IntegerType"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "schema = StructType([\n    StructField('x', IntegerType(), True),\n    StructField('y', IntegerType(), True),\n    StructField('z', IntegerType(), True)\n])"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'Pour_water',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import os\n\nfile_list = os.listdir('HMP_Dataset')\nfile_list_filtered = [s for s in file_list if '_' in s]\nfile_list_filtered"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df = None\n\nfrom pyspark.sql.functions import lit\n\nfor category in file_list_filtered:\n    data_files = os.listdir('HMP_Dataset/'+category)\n    \n    for data_file in data_files:\n        \n        temp_df = spark.read.option('header','false').option('delimiter', ' ').csv('HMP_Dataset/'+category+'/'+data_file, schema = schema)\n        \n        temp_df = temp_df.withColumn('class',lit(category))\n        \n        temp_df = temp_df.withColumn('source', lit(data_file))\n        \n        if df is None:\n            df = temp_df\n        else:\n            df = df.union(temp_df)\n            "
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "ValueError", 
                    "evalue": "Invalid input: bucket_name is required!", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-11-f738d25883e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark_test.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/ibmos2spark/osconfig.py\u001b[0m in \u001b[0;36murl\u001b[0;34m(self, object_name, bucket_name)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mbucket_name_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid input: bucket_name is required!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# use service name that we set up hadoop config for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mValueError\u001b[0m: Invalid input: bucket_name is required!"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df.show()"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+----------+\n|  x|  y|  z|      class|              source|classIndex|\n+---+---+---+-----------+--------------------+----------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|\n+---+---+---+-----------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# String Indexer\n\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol = 'class', outputCol='classIndex')\n\nindexed = indexer.fit(df).transform(df)\n\nindexed.show()"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+\n|  x|  y|  z|      class|              source|classIndex|   CategoryVec|\n+---+---+---+-----------+--------------------+----------+--------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n+---+---+---+-----------+--------------------+----------+--------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# One-Hot Encoding\n\nfrom pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'CategoryVec')\n\nencoded = encoder.transform(indexed)\n\nencoded.show()"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n|  x|  y|  z|      class|              source|classIndex|   CategoryVec|        features|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# convert columns to vector\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler(inputCols=['x','y','z'], outputCol='features')\n\nfeatures_vectorized = vectorAssembler.transform(encoded)\n\nfeatures_vectorized.show()"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   CategoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# Normalize \n\nfrom pyspark.ml.feature import Normalizer\n\nnormalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\n\nnormalized_data = normalizer.transform(features_vectorized)\n\nnormalized_data.show()"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------+--------------------+\n|   CategoryVec|       features_norm|\n+--------------+--------------------+\n|(12,[5],[1.0])|[0.20754716981132...|\n|(12,[5],[1.0])|[0.20754716981132...|\n|(12,[5],[1.0])|[0.20183486238532...|\n|(12,[5],[1.0])|[0.20183486238532...|\n|(12,[5],[1.0])|[0.19626168224299...|\n|(12,[5],[1.0])|[0.20560747663551...|\n|(12,[5],[1.0])|[0.19047619047619...|\n|(12,[5],[1.0])|[0.20370370370370...|\n|(12,[5],[1.0])|[0.20754716981132...|\n|(12,[5],[1.0])|[0.20370370370370...|\n|(12,[5],[1.0])|[0.2,0.4857142857...|\n|(12,[5],[1.0])|[0.19230769230769...|\n|(12,[5],[1.0])|[0.20388349514563...|\n|(12,[5],[1.0])|[0.20388349514563...|\n|(12,[5],[1.0])|[0.18867924528301...|\n|(12,[5],[1.0])|[0.17821782178217...|\n|(12,[5],[1.0])|[0.18811881188118...|\n|(12,[5],[1.0])|[0.15533980582524...|\n|(12,[5],[1.0])|[0.17142857142857...|\n|(12,[5],[1.0])|[0.17821782178217...|\n+--------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# Pipeline\n\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [indexer, encoder, vectorAssembler, normalizer])\n\nmodel = pipeline.fit(df)\nprediction = model.transform(df)\n\n#remove unwanted columns\n\ndf_train = prediction.drop('x').drop('y').drop('z').drop('class').drop('source').drop('classIndex').drop('features')\n\ndf_train.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}